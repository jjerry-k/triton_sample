version: "2.3"
services:
  tritonserver:
    image: nvcr.io/nvidia/tritonserver:23.10-py3
    volumes: 
      - ./model_repository:/models
    ports:
      - 8020:8000
      - 8021:8001
      - 8022:8002
    networks:
      - backend
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    command: tritonserver --model-repository=/models

  api:
    build:
      dockerfile: Dockerfile.api
      context: ./api
    user: "${UID}:${GID}"
    volumes:
      - ./api:/api
    environment: 
      - TZ=Asia/Seoul
    ports:
      - "8080:8080"
    networks:
      - backend
    command: uvicorn main:app --host 0.0.0.0 --port 8080
networks: 
  backend:
    driver: bridge
